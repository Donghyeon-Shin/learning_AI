{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57efd8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transfrom = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32 # 배치 크기\n",
    "\n",
    "# 데이터셋 다운로드\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transfrom)\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 데이터 증강\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# 검증/테스트 데이터\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 데이터셋 다운로드\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c233c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Train/Validation 분할 (80:20)\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "\n",
    "datasets = random_split(full_trainset, [train_size, val_size])\n",
    "trainset = datasets[0]  # 학습용\n",
    "valset = datasets[1]     # 검증용\n",
    "\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "# DataLoader : 데이터셋을 배치 단위로 관리하는 역할\n",
    "    # batch_size : 배치 크기\n",
    "    # shuffle : 데이터를 섞을지 여부\n",
    "    # num_workers : 데이터 로드에 사용할 쓰레드 수\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18049a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# SEBlock : Squeeze-and-Excitation Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1) # Squeeze\n",
    "\n",
    "        # Excitation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        z = self.avg_pool(x).view(b, c) # 딥러닝에서는 이미지를 batch 수 만큼 한번에 진행\n",
    "        s = self.fc(z).view(b, c, 1, 1) # 채널 수 만큼 출력\n",
    "        return x * s # scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae55adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNetBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, use_se=False, reduction=16):\n",
    "        super(ResNetBottleneck, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        # SEBlock 추가 여부\n",
    "        if use_se:\n",
    "            self.se = SEBlock(planes * self.expansion, reduction)\n",
    "        else:\n",
    "            self.se = nn.Identity() # SEBlock 추가 안할 경우 그냥 통과\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = self.se(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc0e56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, use_se=False, reduction=16):\n",
    "        super(Net, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # 채널 수 3 -> 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # _make_layer 함수 호출하여 여러 개의 블록 그룹 생성\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], 1, use_se, reduction)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], 2, use_se, reduction)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], 2, use_se, reduction)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], 2, False, reduction) # 마지막 블록은 SEBlock 추가 안함\n",
    "\n",
    "        # average pooling 레이어 사용\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # 완전 연결 레이어(Fully Connected Layer) 512 * block.expansion -> 10개의 클래스로 분류\n",
    "        self.fc = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, use_se=False, reduction=16):\n",
    "        strides = [stride] + [1] * (num_blocks - 1) # 첫 번째 블록은 stride, 나머지는 1\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, use_se, reduction))\n",
    "            self.in_planes = planes * block.expansion # 채널 수 증가\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e847605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 layer resnet\n",
    "resnet_50 = Net(ResNetBottleneck, [3, 4, 6, 3])\n",
    "resnet_50_loss = []\n",
    "resnet_50_error = []\n",
    "\n",
    "se_resnet_50 = Net(ResNetBottleneck, [3, 4, 6, 3], use_se=True, reduction=16)\n",
    "se_resnet_50_loss = []\n",
    "se_resnet_50_error = []\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "resnet_50 = resnet_50.to(device)\n",
    "se_resnet_50 = se_resnet_50.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "344a1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 크로스 엔트로피 손실 함수 사용\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 초기 학습률을 더 낮게 설정 (경사 폭발 방지)\n",
    "# ResNet-50과 같은 깊은 네트워크에서는 더 보수적인 접근 필요\n",
    "\n",
    "# 모멘텀 0.9, weight_decay 1e-4\n",
    "resnet_50_optimizer = optim.SGD(resnet_50.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "se_resnet_50_optimizer = optim.SGD(se_resnet_50.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# 학습률 스케줄링\n",
    "resnet_50_scheduler = optim.lr_scheduler.ReduceLROnPlateau(resnet_50_optimizer, mode='min', factor=0.1, patience=5)\n",
    "se_resnet_50_scheduler = optim.lr_scheduler.ReduceLROnPlateau(se_resnet_50_optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "# 20번 에포크 학습\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc984c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 (top1 error rate, top5 error rate)\n",
    "def evaluate_model(model, data_loader, is_top5=False):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)  # GPU로 이동\n",
    "            labels = labels.to(device)  # GPU로 이동\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if is_top5:\n",
    "                _, top5 = outputs.topk(5, dim=1)\n",
    "                total_correct += (top5 == labels.unsqueeze(1)).any(dim=1).sum().item()\n",
    "            else:\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return (1 - total_correct / total_samples) * 100           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b3314e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 2.2631, Error = 75.99%\n",
      "Batch 200: SE-ResNet50 Loss = 2.1891, Error = 73.93%\n",
      "Batch 300: SE-ResNet50 Loss = 1.9621, Error = 70.32%\n",
      "Batch 400: SE-ResNet50 Loss = 1.8811, Error = 70.68%\n",
      "Batch 500: SE-ResNet50 Loss = 1.8298, Error = 71.09%\n",
      "Batch 600: SE-ResNet50 Loss = 1.7387, Error = 63.33%\n",
      "Batch 700: SE-ResNet50 Loss = 1.6845, Error = 69.06%\n",
      "Batch 800: SE-ResNet50 Loss = 1.7064, Error = 64.08%\n",
      "Batch 900: SE-ResNet50 Loss = 1.6457, Error = 60.02%\n",
      "Batch 1000: SE-ResNet50 Loss = 1.6090, Error = 61.33%\n",
      "Batch 1100: SE-ResNet50 Loss = 1.5986, Error = 59.42%\n",
      "Batch 1200: SE-ResNet50 Loss = 1.5659, Error = 58.15%\n",
      "\n",
      "Epoch 2/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 1.6845, Error = 61.08%\n",
      "Batch 200: SE-ResNet50 Loss = 1.8212, Error = 60.47%\n",
      "Batch 300: SE-ResNet50 Loss = 1.6972, Error = 60.92%\n",
      "Batch 400: SE-ResNet50 Loss = 1.5822, Error = 55.54%\n",
      "Batch 500: SE-ResNet50 Loss = 1.5125, Error = 53.38%\n",
      "Batch 600: SE-ResNet50 Loss = 1.5143, Error = 54.19%\n",
      "Batch 700: SE-ResNet50 Loss = 1.4523, Error = 56.96%\n",
      "Batch 800: SE-ResNet50 Loss = 1.4546, Error = 56.29%\n",
      "Batch 900: SE-ResNet50 Loss = 1.4021, Error = 52.39%\n",
      "Batch 1000: SE-ResNet50 Loss = 1.3974, Error = 52.11%\n",
      "Batch 1100: SE-ResNet50 Loss = 1.3902, Error = 50.98%\n",
      "Batch 1200: SE-ResNet50 Loss = 1.3371, Error = 51.18%\n",
      "\n",
      "Epoch 3/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 1.4556, Error = 54.86%\n",
      "Batch 200: SE-ResNet50 Loss = 1.4527, Error = 52.66%\n",
      "Batch 300: SE-ResNet50 Loss = 1.4248, Error = 50.26%\n",
      "Batch 400: SE-ResNet50 Loss = 1.3195, Error = 49.36%\n",
      "Batch 500: SE-ResNet50 Loss = 1.3495, Error = 49.94%\n",
      "Batch 600: SE-ResNet50 Loss = 1.3219, Error = 47.95%\n",
      "Batch 700: SE-ResNet50 Loss = 1.2863, Error = 47.34%\n",
      "Batch 800: SE-ResNet50 Loss = 1.2632, Error = 45.00%\n",
      "Batch 900: SE-ResNet50 Loss = 1.2686, Error = 44.59%\n",
      "Batch 1000: SE-ResNet50 Loss = 1.2671, Error = 44.95%\n",
      "Batch 1100: SE-ResNet50 Loss = 1.2014, Error = 43.25%\n",
      "Batch 1200: SE-ResNet50 Loss = 1.2146, Error = 44.68%\n",
      "\n",
      "Epoch 4/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 1.3194, Error = 43.58%\n",
      "Batch 200: SE-ResNet50 Loss = 1.2533, Error = 43.03%\n",
      "Batch 300: SE-ResNet50 Loss = 1.1802, Error = 46.67%\n",
      "Batch 400: SE-ResNet50 Loss = 1.2160, Error = 43.24%\n",
      "Batch 500: SE-ResNet50 Loss = 1.1589, Error = 44.28%\n",
      "Batch 600: SE-ResNet50 Loss = 1.1482, Error = 42.48%\n",
      "Batch 700: SE-ResNet50 Loss = 1.1494, Error = 41.77%\n",
      "Batch 800: SE-ResNet50 Loss = 1.1351, Error = 40.39%\n",
      "Batch 900: SE-ResNet50 Loss = 1.1286, Error = 39.81%\n",
      "Batch 1000: SE-ResNet50 Loss = 1.1078, Error = 38.94%\n",
      "Batch 1100: SE-ResNet50 Loss = 1.0998, Error = 40.29%\n",
      "Batch 1200: SE-ResNet50 Loss = 1.0910, Error = 38.17%\n",
      "\n",
      "Epoch 5/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 1.1633, Error = 40.21%\n",
      "Batch 200: SE-ResNet50 Loss = 1.1329, Error = 39.43%\n",
      "Batch 300: SE-ResNet50 Loss = 1.0505, Error = 38.72%\n",
      "Batch 400: SE-ResNet50 Loss = 1.0498, Error = 36.82%\n",
      "Batch 500: SE-ResNet50 Loss = 1.0401, Error = 38.27%\n",
      "Batch 600: SE-ResNet50 Loss = 1.0485, Error = 36.81%\n",
      "Batch 700: SE-ResNet50 Loss = 1.0426, Error = 42.51%\n",
      "Batch 800: SE-ResNet50 Loss = 1.0261, Error = 37.94%\n",
      "Batch 900: SE-ResNet50 Loss = 1.0830, Error = 36.07%\n",
      "Batch 1000: SE-ResNet50 Loss = 1.0434, Error = 33.87%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.9942, Error = 36.66%\n",
      "Batch 1200: SE-ResNet50 Loss = 1.0182, Error = 34.55%\n",
      "\n",
      "Epoch 6/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 1.0434, Error = 40.37%\n",
      "Batch 200: SE-ResNet50 Loss = 1.0091, Error = 32.79%\n",
      "Batch 300: SE-ResNet50 Loss = 0.9817, Error = 33.41%\n",
      "Batch 400: SE-ResNet50 Loss = 0.9807, Error = 34.25%\n",
      "Batch 500: SE-ResNet50 Loss = 0.9689, Error = 35.66%\n",
      "Batch 600: SE-ResNet50 Loss = 0.9622, Error = 32.74%\n",
      "Batch 700: SE-ResNet50 Loss = 0.9463, Error = 35.57%\n",
      "Batch 800: SE-ResNet50 Loss = 0.9019, Error = 34.02%\n",
      "Batch 900: SE-ResNet50 Loss = 0.9371, Error = 36.35%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.9264, Error = 32.52%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.9008, Error = 32.28%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.9241, Error = 30.99%\n",
      "\n",
      "Epoch 7/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 1.0297, Error = 31.74%\n",
      "Batch 200: SE-ResNet50 Loss = 0.9600, Error = 32.91%\n",
      "Batch 300: SE-ResNet50 Loss = 0.8929, Error = 32.81%\n",
      "Batch 400: SE-ResNet50 Loss = 0.8986, Error = 32.32%\n",
      "Batch 500: SE-ResNet50 Loss = 0.8796, Error = 34.20%\n",
      "Batch 600: SE-ResNet50 Loss = 0.9077, Error = 31.28%\n",
      "Batch 700: SE-ResNet50 Loss = 0.8599, Error = 32.83%\n",
      "Batch 800: SE-ResNet50 Loss = 0.9070, Error = 32.51%\n",
      "Batch 900: SE-ResNet50 Loss = 0.8737, Error = 30.14%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.8712, Error = 32.66%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.8477, Error = 29.82%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.8078, Error = 31.61%\n",
      "\n",
      "Epoch 8/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.9309, Error = 29.65%\n",
      "Batch 200: SE-ResNet50 Loss = 0.8596, Error = 29.48%\n",
      "Batch 300: SE-ResNet50 Loss = 0.8790, Error = 33.40%\n",
      "Batch 400: SE-ResNet50 Loss = 0.8357, Error = 29.26%\n",
      "Batch 500: SE-ResNet50 Loss = 0.8434, Error = 33.20%\n",
      "Batch 600: SE-ResNet50 Loss = 0.8115, Error = 28.82%\n",
      "Batch 700: SE-ResNet50 Loss = 0.8166, Error = 28.87%\n",
      "Batch 800: SE-ResNet50 Loss = 0.8161, Error = 29.27%\n",
      "Batch 900: SE-ResNet50 Loss = 0.7645, Error = 30.26%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.8239, Error = 30.77%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.8341, Error = 30.47%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.8086, Error = 28.97%\n",
      "\n",
      "Epoch 9/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.8870, Error = 30.21%\n",
      "Batch 200: SE-ResNet50 Loss = 0.8113, Error = 27.87%\n",
      "Batch 300: SE-ResNet50 Loss = 0.8036, Error = 29.10%\n",
      "Batch 400: SE-ResNet50 Loss = 0.7779, Error = 27.45%\n",
      "Batch 500: SE-ResNet50 Loss = 0.8007, Error = 26.95%\n",
      "Batch 600: SE-ResNet50 Loss = 0.7702, Error = 30.01%\n",
      "Batch 700: SE-ResNet50 Loss = 0.7687, Error = 30.46%\n",
      "Batch 800: SE-ResNet50 Loss = 0.7937, Error = 25.97%\n",
      "Batch 900: SE-ResNet50 Loss = 0.7650, Error = 29.06%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.7809, Error = 27.65%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.7748, Error = 31.64%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.7668, Error = 28.80%\n",
      "\n",
      "Epoch 10/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.7978, Error = 28.55%\n",
      "Batch 200: SE-ResNet50 Loss = 0.7653, Error = 28.07%\n",
      "Batch 300: SE-ResNet50 Loss = 0.7831, Error = 28.91%\n",
      "Batch 400: SE-ResNet50 Loss = 0.7612, Error = 27.75%\n",
      "Batch 500: SE-ResNet50 Loss = 0.7411, Error = 28.67%\n",
      "Batch 600: SE-ResNet50 Loss = 0.7041, Error = 29.77%\n",
      "Batch 700: SE-ResNet50 Loss = 0.7559, Error = 27.79%\n",
      "Batch 800: SE-ResNet50 Loss = 0.7871, Error = 28.91%\n",
      "Batch 900: SE-ResNet50 Loss = 0.7339, Error = 27.43%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.7328, Error = 25.62%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.7106, Error = 27.58%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.7370, Error = 28.69%\n",
      "\n",
      "Epoch 11/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.8102, Error = 29.29%\n",
      "Batch 200: SE-ResNet50 Loss = 0.7558, Error = 26.43%\n",
      "Batch 300: SE-ResNet50 Loss = 0.7245, Error = 26.62%\n",
      "Batch 400: SE-ResNet50 Loss = 0.7324, Error = 28.33%\n",
      "Batch 500: SE-ResNet50 Loss = 0.6984, Error = 25.60%\n",
      "Batch 600: SE-ResNet50 Loss = 0.7146, Error = 25.85%\n",
      "Batch 700: SE-ResNet50 Loss = 0.7067, Error = 26.60%\n",
      "Batch 800: SE-ResNet50 Loss = 0.6880, Error = 26.07%\n",
      "Batch 900: SE-ResNet50 Loss = 0.7021, Error = 25.39%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.6803, Error = 27.78%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.6775, Error = 26.21%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.6990, Error = 24.81%\n",
      "\n",
      "Epoch 12/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.6934, Error = 27.97%\n",
      "Batch 200: SE-ResNet50 Loss = 0.6794, Error = 27.45%\n",
      "Batch 300: SE-ResNet50 Loss = 0.6972, Error = 23.54%\n",
      "Batch 400: SE-ResNet50 Loss = 0.6916, Error = 25.96%\n",
      "Batch 500: SE-ResNet50 Loss = 0.6658, Error = 24.22%\n",
      "Batch 600: SE-ResNet50 Loss = 0.6717, Error = 24.64%\n",
      "Batch 700: SE-ResNet50 Loss = 0.6708, Error = 24.30%\n",
      "Batch 800: SE-ResNet50 Loss = 0.6491, Error = 23.38%\n",
      "Batch 900: SE-ResNet50 Loss = 0.6666, Error = 24.43%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.6713, Error = 25.53%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.6988, Error = 25.31%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.6628, Error = 25.82%\n",
      "\n",
      "Epoch 13/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.7273, Error = 24.79%\n",
      "Batch 200: SE-ResNet50 Loss = 0.6477, Error = 24.95%\n",
      "Batch 300: SE-ResNet50 Loss = 0.6226, Error = 24.28%\n",
      "Batch 400: SE-ResNet50 Loss = 0.6382, Error = 23.95%\n",
      "Batch 500: SE-ResNet50 Loss = 0.6288, Error = 23.51%\n",
      "Batch 600: SE-ResNet50 Loss = 0.6325, Error = 25.14%\n",
      "Batch 700: SE-ResNet50 Loss = 0.6395, Error = 24.65%\n",
      "Batch 800: SE-ResNet50 Loss = 0.6534, Error = 25.31%\n",
      "Batch 900: SE-ResNet50 Loss = 0.6601, Error = 24.34%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.6274, Error = 25.40%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.6424, Error = 23.28%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.6723, Error = 23.75%\n",
      "\n",
      "Epoch 14/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.6797, Error = 24.50%\n",
      "Batch 200: SE-ResNet50 Loss = 0.6722, Error = 22.43%\n",
      "Batch 300: SE-ResNet50 Loss = 0.6350, Error = 26.33%\n",
      "Batch 400: SE-ResNet50 Loss = 0.6324, Error = 23.79%\n",
      "Batch 500: SE-ResNet50 Loss = 0.6179, Error = 21.72%\n",
      "Batch 600: SE-ResNet50 Loss = 0.5820, Error = 23.91%\n",
      "Batch 700: SE-ResNet50 Loss = 0.5809, Error = 23.52%\n",
      "Batch 800: SE-ResNet50 Loss = 0.6523, Error = 22.76%\n",
      "Batch 900: SE-ResNet50 Loss = 0.5859, Error = 23.22%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.6019, Error = 21.70%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.5776, Error = 23.68%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.6056, Error = 22.80%\n",
      "\n",
      "Epoch 15/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.6576, Error = 24.11%\n",
      "Batch 200: SE-ResNet50 Loss = 0.5743, Error = 21.37%\n",
      "Batch 300: SE-ResNet50 Loss = 0.5837, Error = 22.54%\n",
      "Batch 400: SE-ResNet50 Loss = 0.5872, Error = 23.41%\n",
      "Batch 500: SE-ResNet50 Loss = 0.5855, Error = 22.05%\n",
      "Batch 600: SE-ResNet50 Loss = 0.5658, Error = 21.80%\n",
      "Batch 700: SE-ResNet50 Loss = 0.5893, Error = 21.70%\n",
      "Batch 800: SE-ResNet50 Loss = 0.5682, Error = 21.36%\n",
      "Batch 900: SE-ResNet50 Loss = 0.5783, Error = 23.96%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.5600, Error = 22.37%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.5521, Error = 21.89%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.5564, Error = 21.12%\n",
      "\n",
      "Epoch 16/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.6617, Error = 21.76%\n",
      "Batch 200: SE-ResNet50 Loss = 0.5828, Error = 21.59%\n",
      "Batch 300: SE-ResNet50 Loss = 0.5368, Error = 20.77%\n",
      "Batch 400: SE-ResNet50 Loss = 0.5555, Error = 20.91%\n",
      "Batch 500: SE-ResNet50 Loss = 0.5354, Error = 21.40%\n",
      "Batch 600: SE-ResNet50 Loss = 0.5555, Error = 21.41%\n",
      "Batch 700: SE-ResNet50 Loss = 0.5575, Error = 20.62%\n",
      "Batch 800: SE-ResNet50 Loss = 0.5216, Error = 21.41%\n",
      "Batch 900: SE-ResNet50 Loss = 0.5658, Error = 21.37%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.5785, Error = 20.78%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.5103, Error = 21.87%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.5363, Error = 21.95%\n",
      "\n",
      "Epoch 17/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.6196, Error = 20.80%\n",
      "Batch 200: SE-ResNet50 Loss = 0.5317, Error = 21.04%\n",
      "Batch 300: SE-ResNet50 Loss = 0.5186, Error = 19.77%\n",
      "Batch 400: SE-ResNet50 Loss = 0.5371, Error = 23.22%\n",
      "Batch 500: SE-ResNet50 Loss = 0.5398, Error = 20.79%\n",
      "Batch 600: SE-ResNet50 Loss = 0.5415, Error = 20.72%\n",
      "Batch 700: SE-ResNet50 Loss = 0.5079, Error = 21.99%\n",
      "Batch 800: SE-ResNet50 Loss = 0.5148, Error = 20.65%\n",
      "Batch 900: SE-ResNet50 Loss = 0.5263, Error = 20.72%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.5199, Error = 19.98%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.5022, Error = 22.40%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.5354, Error = 21.51%\n",
      "\n",
      "Epoch 18/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.6070, Error = 24.08%\n",
      "Batch 200: SE-ResNet50 Loss = 0.5156, Error = 19.72%\n",
      "Batch 300: SE-ResNet50 Loss = 0.5172, Error = 19.07%\n",
      "Batch 400: SE-ResNet50 Loss = 0.4879, Error = 22.85%\n",
      "Batch 500: SE-ResNet50 Loss = 0.4951, Error = 21.76%\n",
      "Batch 600: SE-ResNet50 Loss = 0.5089, Error = 20.05%\n",
      "Batch 700: SE-ResNet50 Loss = 0.4877, Error = 19.64%\n",
      "Batch 800: SE-ResNet50 Loss = 0.4755, Error = 18.55%\n",
      "Batch 900: SE-ResNet50 Loss = 0.5224, Error = 20.60%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.5249, Error = 19.74%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.4774, Error = 20.96%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.5069, Error = 21.41%\n",
      "\n",
      "Epoch 19/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.5319, Error = 20.01%\n",
      "Batch 200: SE-ResNet50 Loss = 0.5369, Error = 21.40%\n",
      "Batch 300: SE-ResNet50 Loss = 0.4603, Error = 20.10%\n",
      "Batch 400: SE-ResNet50 Loss = 0.4562, Error = 20.20%\n",
      "Batch 500: SE-ResNet50 Loss = 0.4681, Error = 19.43%\n",
      "Batch 600: SE-ResNet50 Loss = 0.4806, Error = 19.79%\n",
      "Batch 700: SE-ResNet50 Loss = 0.4991, Error = 18.59%\n",
      "Batch 800: SE-ResNet50 Loss = 0.4528, Error = 19.74%\n",
      "Batch 900: SE-ResNet50 Loss = 0.4962, Error = 19.46%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.4647, Error = 19.34%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.4892, Error = 19.48%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.4869, Error = 19.50%\n",
      "\n",
      "Epoch 20/20\n",
      "Current LR - SE-ResNet50: 0.001000\n",
      "Batch 100: SE-ResNet50 Loss = 0.5251, Error = 18.76%\n",
      "Batch 200: SE-ResNet50 Loss = 0.4859, Error = 18.29%\n",
      "Batch 300: SE-ResNet50 Loss = 0.4659, Error = 18.88%\n",
      "Batch 400: SE-ResNet50 Loss = 0.4742, Error = 18.25%\n",
      "Batch 500: SE-ResNet50 Loss = 0.4515, Error = 18.53%\n",
      "Batch 600: SE-ResNet50 Loss = 0.4512, Error = 19.66%\n",
      "Batch 700: SE-ResNet50 Loss = 0.4482, Error = 19.20%\n",
      "Batch 800: SE-ResNet50 Loss = 0.4584, Error = 20.41%\n",
      "Batch 900: SE-ResNet50 Loss = 0.4621, Error = 20.08%\n",
      "Batch 1000: SE-ResNet50 Loss = 0.4517, Error = 19.44%\n",
      "Batch 1100: SE-ResNet50 Loss = 0.4763, Error = 18.94%\n",
      "Batch 1200: SE-ResNet50 Loss = 0.4771, Error = 18.80%\n",
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "# SE-ResNet-50 model 학습 (스케줄링 적용)\n",
    "for epoch in range(num_epochs):\n",
    "    # 모델을 학습 모드로 설정\n",
    "    se_resnet_50.train()\n",
    "    \n",
    "    running_loss_se_resnet_50 = 0.0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Current LR - SE-ResNet50: {se_resnet_50_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)  # GPU로 이동\n",
    "        labels = labels.to(device)  # GPU로 이동\n",
    "\n",
    "        # SE-ResNet-50 학습\n",
    "        se_resnet_50_optimizer.zero_grad()\n",
    "        outputs_se_resnet_50 = se_resnet_50(inputs)\n",
    "        loss_se_resnet_50 = criterion(outputs_se_resnet_50, labels)\n",
    "        loss_se_resnet_50.backward()\n",
    "        se_resnet_50_optimizer.step()\n",
    "        \n",
    "\n",
    "        # 배치 손실 합계 계산\n",
    "        running_loss_se_resnet_50 += loss_se_resnet_50.item()\n",
    "\n",
    "        # 100 배치마다 검증 및 출력\n",
    "        if (i + 1) % 100 == 0:\n",
    "            val_error_se_resnet_50 = evaluate_model(se_resnet_50, val_loader)\n",
    "\n",
    "            se_resnet_50_error.append(val_error_se_resnet_50)\n",
    "\n",
    "            avg_loss_se_resnet_50 = running_loss_se_resnet_50 / 100\n",
    "            \n",
    "            se_resnet_50_loss.append(avg_loss_se_resnet_50)\n",
    "\n",
    "            print(f\"Batch {i + 1}: SE-ResNet50 Loss = {avg_loss_se_resnet_50:.4f}, Error = {val_error_se_resnet_50:.2f}%\")\n",
    "            \n",
    "            # running_loss 초기화\n",
    "            running_loss_se_resnet_50 = 0.0\n",
    "    \n",
    "    # 에포크 끝에서 스케줄러 업데이트 (검증 오류율 기준)\n",
    "    if len(se_resnet_50_error) > 0:\n",
    "        se_resnet_50_scheduler.step(se_resnet_50_error[-1])\n",
    "\n",
    "print(\"학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "Current LR - ResNet50: 0.001000\n",
      "Batch 100: ResNet50 Loss = 2.3851, Error = 75.33%\n",
      "Batch 200: ResNet50 Loss = 2.1475, Error = 79.39%\n",
      "Batch 300: ResNet50 Loss = 2.0202, Error = 71.74%\n",
      "Batch 400: ResNet50 Loss = 1.9066, Error = 71.41%\n",
      "Batch 500: ResNet50 Loss = 1.8452, Error = 68.24%\n",
      "Batch 600: ResNet50 Loss = 1.7856, Error = 64.99%\n",
      "Batch 700: ResNet50 Loss = 1.7570, Error = 65.96%\n",
      "Batch 800: ResNet50 Loss = 1.7672, Error = 63.48%\n",
      "Batch 900: ResNet50 Loss = 1.6554, Error = 61.32%\n",
      "Batch 1000: ResNet50 Loss = 1.6462, Error = 62.57%\n",
      "Batch 1100: ResNet50 Loss = 1.6521, Error = 61.57%\n",
      "Batch 1200: ResNet50 Loss = 1.6032, Error = 61.94%\n",
      "\n",
      "Epoch 2/100\n",
      "Current LR - ResNet50: 0.001000\n",
      "Batch 100: ResNet50 Loss = 1.7759, Error = 61.44%\n",
      "Batch 200: ResNet50 Loss = 1.9091, Error = 62.93%\n",
      "Batch 300: ResNet50 Loss = 1.6761, Error = 64.79%\n",
      "Batch 400: ResNet50 Loss = 1.6089, Error = 58.88%\n",
      "Batch 500: ResNet50 Loss = 1.5590, Error = 60.20%\n",
      "Batch 600: ResNet50 Loss = 1.5876, Error = 58.11%\n",
      "Batch 700: ResNet50 Loss = 1.5617, Error = 60.18%\n",
      "Batch 800: ResNet50 Loss = 1.5196, Error = 60.58%\n",
      "Batch 900: ResNet50 Loss = 1.5017, Error = 60.06%\n",
      "Batch 1000: ResNet50 Loss = 1.4991, Error = 55.70%\n",
      "Batch 1100: ResNet50 Loss = 1.4670, Error = 50.85%\n",
      "Batch 1200: ResNet50 Loss = 1.3989, Error = 49.14%\n",
      "\n",
      "Epoch 3/100\n",
      "Current LR - ResNet50: 0.001000\n",
      "Batch 100: ResNet50 Loss = 1.5697, Error = 56.25%\n",
      "Batch 200: ResNet50 Loss = 1.5978, Error = 57.20%\n",
      "Batch 300: ResNet50 Loss = 1.5465, Error = 54.78%\n",
      "Batch 400: ResNet50 Loss = 1.4420, Error = 53.16%\n",
      "Batch 500: ResNet50 Loss = 1.3604, Error = 49.46%\n",
      "Batch 600: ResNet50 Loss = 1.3681, Error = 51.98%\n",
      "Batch 700: ResNet50 Loss = 1.3986, Error = 49.26%\n",
      "Batch 800: ResNet50 Loss = 1.3357, Error = 46.10%\n",
      "Batch 900: ResNet50 Loss = 1.3380, Error = 46.60%\n",
      "Batch 1000: ResNet50 Loss = 1.3377, Error = 49.35%\n",
      "Batch 1100: ResNet50 Loss = 1.2926, Error = 46.07%\n",
      "Batch 1200: ResNet50 Loss = 1.2906, Error = 44.34%\n",
      "\n",
      "Epoch 4/100\n",
      "Current LR - ResNet50: 0.001000\n",
      "Batch 100: ResNet50 Loss = 1.4086, Error = 47.59%\n",
      "Batch 200: ResNet50 Loss = 1.3257, Error = 46.94%\n",
      "Batch 300: ResNet50 Loss = 1.3614, Error = 46.27%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 100 배치마다 검증 및 출력\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     val_error_resnet_50 = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet_50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     resnet_50_error.append(val_error_resnet_50)\n\u001b[32m     33\u001b[39m     avg_loss_resnet_50 = running_loss_resnet_50 / \u001b[32m100\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, data_loader, is_top5)\u001b[39m\n\u001b[32m      5\u001b[39m total_samples = \u001b[32m0\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GPU로 이동\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GPU로 이동\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[39m, in \u001b[36mCIFAR10.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    116\u001b[39m img = Image.fromarray(img)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    122\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\workspace-vom\\env\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:922\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    920\u001b[39m mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n\u001b[32m    921\u001b[39m std = torch.as_tensor(std, dtype=dtype, device=tensor.device)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, leading to division by zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mean.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ResNet-50 model 학습 (스케줄링 적용)\n",
    "for epoch in range(num_epochs):\n",
    "    # 모델을 학습 모드로 설정\n",
    "    resnet_50.train()\n",
    "    \n",
    "    running_loss_resnet_50 = 0.0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Current LR - ResNet50: {resnet_50_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)  # GPU로 이동\n",
    "        labels = labels.to(device)  # GPU로 이동\n",
    "\n",
    "        # ResNet-50 학습\n",
    "        resnet_50_optimizer.zero_grad()\n",
    "        outputs_resnet_50 = resnet_50(inputs)\n",
    "        loss_resnet_50 = criterion(outputs_resnet_50, labels)\n",
    "        loss_resnet_50.backward()\n",
    "        resnet_50_optimizer.step()\n",
    "        \n",
    "\n",
    "        # 배치 손실 합계 계산\n",
    "        running_loss_resnet_50 += loss_resnet_50.item()\n",
    "\n",
    "        # 100 배치마다 검증 및 출력\n",
    "        if (i + 1) % 100 == 0:\n",
    "            val_error_resnet_50 = evaluate_model(resnet_50, val_loader)\n",
    "\n",
    "            resnet_50_error.append(val_error_resnet_50)\n",
    "\n",
    "            avg_loss_resnet_50 = running_loss_resnet_50 / 100\n",
    "            \n",
    "            resnet_50_loss.append(avg_loss_resnet_50)\n",
    "\n",
    "            print(f\"Batch {i + 1}: ResNet50 Loss = {avg_loss_resnet_50:.4f}, Error = {val_error_resnet_50:.2f}%\")\n",
    "            \n",
    "            # running_loss 초기화\n",
    "            running_loss_resnet_50 = 0.0\n",
    "    \n",
    "    # 에포크 끝에서 스케줄러 업데이트 (검증 오류율 기준)\n",
    "    if len(resnet_50_error) > 0:\n",
    "        resnet_50_scheduler.step(resnet_50_error[-1])\n",
    "\n",
    "print(\"학습 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
